import streamlit as st
from manage.thread_manage import ThreadManager
from manage.model_manage import load_model, download_model, prepare_model
import cv2
from typing import Optional
import threading
from collections import deque
from queue import Queue
import plotly.express as px

import time
import torch
import numpy as np
from typing import List
from config import *
from typing import Tuple

import os
import warnings

from queue import Queue
import cv2 
from datetime import datetime
from pathlib import Path 
import torch.nn.functional as F
def kill_model():
    if st.session_state.model is not None:
        del st.session_state.model
        st.session_state.model = None

def kill_model_thread(thread_var: ThreadManager):
    if thread_var.thread is not None:
        thread_var.thread_enabled = False
        thread_var.thread.join()
        thread_var.thread = None

    capture = st.session_state.video_capture
    if capture is not None:
        capture.release()
        st.session_state.video_capture = None


def softmax(x):
    y = np.exp(x - np.max(x))
    return y / np.sum(y)


def run_model(model: object, frames: torch.Tensor, thread_manager: ThreadManager) -> Tuple[np.ndarray, np.ndarray]:
    """
    ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ í”„ë ˆì„ ë²¡í„°ì™€ í…ìŠ¤íŠ¸ ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        model (object): ì‚¬ìš©í•  ëª¨ë¸ ê°ì²´
        frames (torch.Tensor): ì²˜ë¦¬í•  í”„ë ˆì„ í…ì„œ
        thread_manager (ThreadManager): ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ThreadManager ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Tuple[np.ndarray, np.ndarray]: ìœ ì‚¬ë„ ì ìˆ˜ ë°°ì—´ê³¼ ì†Œí”„íŠ¸ë§¥ìŠ¤ ê²°ê³¼ ë°°ì—´
    """
    print(f"frames type: {type(frames)}")
    print(f"frames dtype: {frames.dtype}")
    print(f"frames shape: {frames.shape}")
    print(f"frames device: {frames.device}")
    frames = frames.cuda() 
    print(f"frames device???: {frames.device}")
    
     # framesë¥¼ GPUë¡œ ì´ë™
    txt_vectors = thread_manager.text_vectors  # í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ThreadManagerë¡œë¶€í„° ê°€ì ¸ì˜´

    vid_vector = thread_manager.model(video = frames)
    print("ë¹„ë””ì˜¤ ë²¡í„°:" ,vid_vector.shape, "  í…ìŠ¤íŠ¸ ë²¡í„° :", txt_vectors.shape)
    sim_scores = thread_manager.model.model._loose_similarity(sequence_output=txt_vectors, visual_output=vid_vector)
    print(f"Original shape: {sim_scores.shape}") 
    print(f"Original shape: {sim_scores.device}") 
    print(f"Original shape: {type(sim_scores)}") 
    print(f"Original shape: {sim_scores.dtype}") 

    # ìµœëŒ€ê°’ ê³„ì‚°
    max_values, max_indices = sim_scores.max(dim=1)
    print(f"Max values: {max_values.shape}") 

    softmax_values = F.softmax(max_values, dim=0)
    print(f"Max values22222: {softmax_values.shape}") 

    # í…ì„œë¥¼ CPUë¡œ ì´ë™ í›„ NumPy ë°°ì—´ë¡œ ë³€í™˜
    sim_scores_np = sim_scores.cpu().numpy()
    softmax_values_np = softmax_values.cpu().numpy()

    # NumPy ë°°ì—´ë¡œ ë³€í™˜ëœ ê°’ ë¦¬í„´
    return sim_scores_np, softmax_values_np

def model_loop_thread(thread_manager: ThreadManager) -> None:
    """
    ëª¨ë¸ ë£¨í”„ ìŠ¤ë ˆë“œ í•¨ìˆ˜.
    í”„ë ˆì„ì„ ê°€ì ¸ì™€ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³ , ìœ ì‚¬ë„ ì ìˆ˜ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    Args:
        thread_manager (ThreadManager): ìŠ¤ë ˆë“œì™€ ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ThreadManager ì¸ìŠ¤í„´ìŠ¤

    Returns:
        None
    """
    prompts_list = [prompt for category_id, prompt_list in PROMPT_TEXT.items() for prompt in prompt_list]
    print("ì—¬ê¸°ë¶€í„° ëª¨ë¸ ìŠ¤ë ˆë“œ ë¡œê·¸ ì‹œì‘ì…ë‹ˆë‹¤! ")

    while thread_manager.thread_enabled:
        # if thread_manager.frame_queue_updated:
        #     thread_manager.frame_queue_updated = False
        # else:
        #     time.sleep(0.02)  # í”„ë ˆì„ì´ ì—…ë°ì´íŠ¸ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        #     continue

        # í”„ë ˆì„ í ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ëŒ€ê¸°
        thread_manager.frame_queue_updated_event.wait()

        with thread_manager.thread_lock:
            # íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
            frames = torch.tensor(np.array(thread_manager.frame_queue))
            print("ì—¬ê¸° ì°¨ì› ë´ì•¼ë¼! ",frames.shape)

        # í…ì„œ ì°¨ì› ìˆœì„œ ë³€ê²½ (batch, channel, height, width, frames)
        frames = frames.permute((1, 0, 2, 3, 4)).contiguous()
        
        # ëª¨ë¸ ì‹¤í–‰
        sim_scores, sim_softmax = run_model(thread_manager.model, frames, thread_manager)
        print("ì–´ì©”ê»€ë°")
        print(sim_scores.shape)
        print(sim_softmax.shape)
        # ê° í”„ë¡¬í”„íŠ¸ì™€ í•´ë‹¹ ìœ ì‚¬ë„ ì ìˆ˜ ë¡œê·¸ ê¸°ë¡
        sim_scores_list = list(sim_scores)
        # demo_sim_logger.log_sim_scores(prompts_list, sim_scores_list)

        with thread_manager.thread_lock:
            # ìœ ì‚¬ë„ ì ìˆ˜ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ê²°ê³¼ ì—…ë°ì´íŠ¸
            thread_manager.out_sim_scores = sim_scores
            thread_manager.out_sim_softmax = sim_softmax
            print("lockë†“ëŠ”ë‹¤!")


        # ìœ ì‚¬ë„ ì ìˆ˜ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        # update_graph(thread_manager.out_sim_scores)
        # ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œë¥¼ ì•Œë¦¼
        thread_manager.frame_queue_updated_event.clear()
        thread_manager.model_processing_done_event.set()

        time.sleep(0.02)  # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ëŒ€ê¸°



def _calc_tile_cnt(origin_size, tile_size, margin_size):
    tile_cnt = (origin_size - margin_size) / (tile_size - margin_size)
    return int(tile_cnt)

def _make_tiled_images(tile_size: int, frame: np.array, margin_min=0.25) -> Tuple[List[np.array], int, int]:
    """
    ì´ë¯¸ì§€ ë˜ëŠ” í”„ë ˆì„ì„ íƒ€ì¼ í¬ê¸°(tile_size)ë§Œí¼ ë¶„í• í•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ë“¤ì„ ë°˜í™˜.
    
    Args:
        tile_size (int): ê° íƒ€ì¼ì˜ í¬ê¸°.
        frame (np.array): íƒ€ì¼ë¡œ ë¶„í• í•  ì›ë³¸ ì´ë¯¸ì§€ ë˜ëŠ” í”„ë ˆì„.
        margin_min (float): íƒ€ì¼ ê°„ì˜ ìµœì†Œ ì—¬ë°± ë¹„ìœ¨ (ê¸°ë³¸ê°’ì€ 0.25).
    
    Returns:
        Tuple[List[np.array], int, int]:
            - List[np.array]: ìë¥¸ íƒ€ì¼ ì´ë¯¸ì§€ë“¤ì˜ ë¦¬ìŠ¤íŠ¸.
            - int: xì¶• ë°©í–¥ íƒ€ì¼ì˜ ê°œìˆ˜.
            - int: yì¶• ë°©í–¥ íƒ€ì¼ì˜ ê°œìˆ˜.
    """
    h, w = frame.shape[:2]
    margin_size = int(tile_size * margin_min)
    margin_minus_tile_size = tile_size - margin_size

    # íƒ€ì¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    return_frames = []

    # íƒ€ì¼ì´ ì—†ì„ ê²½ìš° ì›ë³¸ í”„ë ˆì„ì„ ë¦¬í„´
    if margin_minus_tile_size * 2 >= h and margin_minus_tile_size * 2 >= w:
        return [frame], 1, 1

    # xì¶•ê³¼ yì¶• íƒ€ì¼ ê°œìˆ˜ ê³„ì‚°
    x_tile_cnt = _calc_tile_cnt(w, tile_size=tile_size, margin_size=margin_size)
    y_tile_cnt = _calc_tile_cnt(h, tile_size=tile_size, margin_size=margin_size)
    end_x_coord = tile_size
    start_x_coord = 0
    y_tile_start_end_coord = []

    # íƒ€ì¼ì„ ìë¥´ëŠ” ë£¨í”„
    for _x in range(x_tile_cnt):
        end_y_coord = tile_size
        start_y_coord = 0
        for _y in range(y_tile_cnt):
            if _x == 0:
                y_tile_start_end_coord.append([start_y_coord, end_y_coord])
            tile = frame[start_y_coord:end_y_coord, start_x_coord:end_x_coord]
            return_frames.append(tile)

            start_y_coord = end_y_coord - margin_size
            end_y_coord = end_y_coord - margin_size + tile_size
        
        # yì¶• ë§ˆì§€ë§‰ íƒ€ì¼
        tile = frame[-tile_size:, start_x_coord:end_x_coord]
        return_frames.append(tile)

        start_x_coord = end_x_coord - margin_size
        end_x_coord = end_x_coord - margin_size + tile_size

    # xì¶• ë§ˆì§€ë§‰ íƒ€ì¼
    for start_y_coord, end_y_coord in y_tile_start_end_coord:
        tile = frame[start_y_coord:end_y_coord, -tile_size:]
        return_frames.append(tile)

    # ëë¶€ë¶„ íƒ€ì¼
    tile = frame[-tile_size:, -tile_size:]
    return_frames.append(tile)

    # íƒ€ì¼ ê°œìˆ˜ ë³´ì •
    x_tile_cnt += 1
    y_tile_cnt += 1

    # ì›ë³¸ í”„ë ˆì„ë„ íƒ€ì¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    return_frames.append(frame)

    # íƒ€ì¼ ë¦¬ìŠ¤íŠ¸ì™€ x, yì¶• íƒ€ì¼ ê°œìˆ˜ ë°˜í™˜
    return return_frames, x_tile_cnt, y_tile_cnt

from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode
from PIL import Image


def process_video( video: np.ndarray, size: int) -> torch.Tensor:
    """
    for each frame in video, to resize, crop, and normalize
    then combine them into one 4D tensor (#frame, 3, size, size)
    @param video: ndarray, (#frame, h, w, 3), rgb
    @param size: int, target pixel size
    """
    process = Compose([
        Resize(size, interpolation=InterpolationMode.BICUBIC),
        CenterCrop(size),
        lambda img: img.convert("RGB"),
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])

    video_tensor = torch.stack([process(Image.fromarray(v)) for v in video], dim=0)
    return video_tensor

def preprocess_input_image(model, origin_image, tile_size, margin_size):
    # tiling 
    tiles, wn, hn = _make_tiled_images(tile_size=tile_size, frame=origin_image, margin_min=margin_size)
    # resize 
    tiles = np.array(process_video(tiles, size=224))
    return tiles, wn, hn


def start_model_thread(model, thread_manager: ThreadManager) -> None:
    """
    ëª¨ë¸ ìŠ¤ë ˆë“œë¥¼ ì‹œì‘í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

    Args:
        model (object): ì‚¬ìš©í•  ëª¨ë¸ ê°ì²´
        thread_manager (ThreadManager): ìŠ¤ë ˆë“œì™€ ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ThreadManager ì¸ìŠ¤í„´ìŠ¤

    Returns:
        None
    """
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    thread_manager.model = model
    thread_manager.frame_queue = deque(maxlen=st.session_state.frame_len)  # session_stateì— ë”°ë¼ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
    thread_manager.frame_queue_updated = False
    thread_manager.thread_enabled = True

    # ëª¨ë¸ ìŠ¤ë ˆë“œ ì‹œì‘
    worker = threading.Thread(target=model_loop_thread, args=(thread_manager,))
    thread_manager.thread = worker
    worker.start()




def combine_image_tiles(tiles, wn, hn):
    tiles_view = np.transpose(tiles, (0, 2, 3, 1))
    h, w = tiles_view.shape[1:3]

    t_min = np.min(tiles_view)
    t_max = np.max(tiles_view)
    tiles_view = ((tiles_view - t_min) / (t_max - t_min) * 255).astype(np.uint8)
    tiles_comb = np.hstack([np.vstack([tiles_view[i * hn + j] for j in range(hn)]) for i in range(wn)])

    # make grid
    tiles_comb[[j for j in range(0, tiles_comb.shape[0], h)] + [-1], :] = [0, 255, 255]
    tiles_comb[:, [i for i in range(0, tiles_comb.shape[1], w)] + [-1]] = [0, 255, 255]
    return tiles_comb

import pandas as pd


def update_text_output(category_num, prompt, sim_score):
    # get component
    category_com, prompt_com, sim_score_com = st.session_state.final_text_output[category_num]

    # get category list
    category_list = st.session_state.prompt_category_list
    text_dict = st.session_state.prompt_text_dict

    # get prompt index
    prompt_idx = text_dict[category_num].index(prompt)
    GAUGE_MIN = st.session_state[f'category_{category_num}_prompt{prompt_idx}_min']
    GAUGE_MAX = st.session_state[f'category_{category_num}_prompt{prompt_idx}_max']

    # scaling sim_score  todo : scaling ê¸°ëŠ¥ í•¨ìˆ˜í™”
    sim_score_scaled = (sim_score - GAUGE_MIN) / (GAUGE_MAX - GAUGE_MIN)

    # update category_com, prompt_com
    prompt_com.write(prompt if sim_score_scaled > 0 else '')
    sim_score_com.progress(min(max(int(sim_score_scaled * 100), 0), 100))

    # Alert logic
    alert_start_time_key = f'alert_start_time_{category_num}'
    event_flag = st.session_state[f'event_flag_{category_num}']
    last_log_index_key = f'last_log_index_{category_num}'

    # Initializing session state
    if f'last_log_index_{category_num}' not in st.session_state:
        st.session_state[f'last_log_index_{category_num}'] = None

    if sim_score_scaled > 0.7:
        category_com.error(category_list[category_num], icon="ğŸš¨")
        current_time = datetime.datetime.now()
        if st.session_state[alert_start_time_key] is None:
            st.session_state[alert_start_time_key] = current_time
        else:
            duration = (current_time - st.session_state[alert_start_time_key]).total_seconds()
            if duration >= 3:
                log_number = st.session_state.log_number
                # Log the event if not already logged
                if not event_flag:
                    st.session_state.logs[log_number] = {
                        'category_num': category_list[category_num],
                        'prompt': prompt,
                        'start_time': st.session_state[alert_start_time_key].strftime('%Y-%m-%d %H:%M:%S'),
                        'duration': f'{duration:.1f}',
                    }
                    st.session_state[last_log_index_key] = log_number
                    st.session_state.log_number += 1
                    st.session_state[f'event_flag_{category_num}'] = True
                else:
                    # update the duration of the ongoing event
                    last_log_index = st.session_state[last_log_index_key]
                    st.session_state.logs[last_log_index]['duration'] = f'{duration:.1f}'

                # Make table
                logs = pd.DataFrame(st.session_state.logs).transpose()
                st.session_state.log_component.table(logs)

    elif sim_score_scaled >= 0.5:
        st.session_state[alert_start_time_key] = None  # Reset start time for new alerts
        st.session_state[f'event_flag_{category_num}'] = False  # Reset event logged flag
        category_com.warning(category_list[category_num], icon="âš ï¸")
    else:
        st.session_state[alert_start_time_key] = None  # Reset start time for new alerts
        st.session_state[f'event_flag_{category_num}'] = False  # Reset event logged flag
        category_com.info(category_list[category_num])

def make_text_output(sim_scores):
    # check sim_scores
    print(f'ì‹œë°œ ì—¬ê¸°ëƒ?!')

    if sim_scores is None:
        print(f'sim_score is None')
        return
    else:
        prompt_all_text_list = st.session_state.prompt_all_text_list
        prompt_text_dict = st.session_state.prompt_text_dict

        # prompt and gauge ìƒì„±
        prompt_gauge_dict = {}

        print(f"Length of sim_scores: {len(sim_scores)}")
        print(f"Length of prompt_all_text_list: {len(st.session_state.prompt_all_text_list)}")



        
        for i, prompt in enumerate(prompt_all_text_list):
            prompt_gauge_dict[prompt] = sim_scores[i]

########################################
        if prompt in prompt_gauge_dict:
            sim_score = prompt_gauge_dict[prompt]
        else:
            print(f"Prompt '{prompt}' not found in prompt_gauge_dict")
########################################
        # ìµœê³ ê°’ ê°±ì‹ 
        for category_num, prompts in prompt_text_dict.items():
            max_similarity_prompt = None
            max_sim_score = 0

            for prompt in prompts:
                sim_score = prompt_gauge_dict[prompt]


############################
                # ë°°ì—´ì¼ ê²½ìš° ìµœëŒ€ê°’ ë˜ëŠ” í‰ê· ê°’ ì„ íƒ (ìµœëŒ€ê°’ì„ ì‚¬ìš© ì˜ˆì‹œ)
                if isinstance(sim_score, np.ndarray):
                    sim_score = sim_score.max()  # ë˜ëŠ” sim_score.mean()ìœ¼ë¡œ í‰ê· ê°’ ì‚¬ìš© ê°€
############################
                if sim_score is None:
                    print(f"sim_score is None for prompt: {prompt}")
                elif not isinstance(sim_score, (int, float)):
                    print(f"sim_score is not a number: {sim_score} for prompt: {prompt}")
##############################

                if sim_score > max_sim_score:
                    max_similarity_prompt = prompt
                    max_sim_score = sim_score

            update_text_output(category_num, max_similarity_prompt, max_sim_score)
            # print(f'Category: {category_num}, prompt: {max_similarity_prompt}, score : {max_sim_score}')


def update_graph(scores, max_length=100):
    if scores is not None:
        # ì´ˆê¸°í™”
        prompt_score_dict = st.session_state.prompt_score_dict
        print("ì‹œë°” ì—¬ê¸°ëƒ?", prompt_score_dict)
        prompts = st.session_state.prompt_all_text_list

        for i, prompt in enumerate(prompts):
            print(f"scores[{i}]: {scores[i]}, type: {type(scores[i])}")
            print(f"scores[{i}] shape: {scores[i].shape}")

            # ë°°ì—´ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            value = np.mean(scores[i])

            if prompt in prompt_score_dict:
                # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ê°€ max_lengthë¥¼ ì´ˆê³¼í•˜ë©´ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                if len(prompt_score_dict[prompt]) >= max_length:
                    prompt_score_dict[prompt].pop(0)
                prompt_score_dict[prompt].append(value)
            else:
                prompt_score_dict[prompt] = [value]

        # ë°ì´í„° ì—…ë°ì´íŠ¸
        st.session_state.prompt_score_dict = prompt_score_dict

    # ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
    graph_component = st.session_state.get('graph_component', None)

    if graph_component is not None:
        fig = px.line(pd.DataFrame(st.session_state.prompt_score_dict), title='Similarity Graph')
        graph_component.plotly_chart(fig, use_container_width=True)


def video_processing_thread(
    video_capture: cv2.VideoCapture,
    thread_var: ThreadManager,
    fps: float,
    frame_int: float,
    tile_size: int,
    tile_margin: float,
    frame_len: int,
    MAX_WIDTH: int,
    MAX_HEIGHT: int,
    delay_state: bool
) -> None:
    """
    ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ í•¨ìˆ˜. ê° í”„ë ˆì„ì„ ì½ê³  ì „ì²˜ë¦¬í•œ í›„ íƒ€ì¼ì„ ìƒì„±í•˜ë©°,
    í”„ë ˆì„ íë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , ê·¸ë˜í”„ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        video_capture (cv2.VideoCapture): ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´.
        thread_var (ThreadManager): ìŠ¤ë ˆë“œ ë° ìƒíƒœ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ThreadManager ê°ì²´.
        fps (float): ë¹„ë””ì˜¤ì˜ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜.
        frame_int (float): í”„ë ˆì„ ê°„ ê°„ê²©.
        tile_size (int): íƒ€ì¼ì˜ í¬ê¸°.
        tile_margin (float): íƒ€ì¼ ê°„ì˜ ì—¬ë°± í¬ê¸°.
        frame_len (int): í”„ë ˆì„ íì˜ ìµœëŒ€ ê¸¸ì´.
        MAX_WIDTH (int): ì¶œë ¥ í”„ë ˆì„ì˜ ìµœëŒ€ ë„ˆë¹„.
        MAX_HEIGHT (int): ì¶œë ¥ í”„ë ˆì„ì˜ ìµœëŒ€ ë†’ì´.
        delay_state (bool): íŒŒì¼ì„ ì½ì„ ë•Œ ë”œë ˆì´ë¥¼ ì„¤ì •í• ì§€ ì—¬ë¶€.

    Returns:
        None
    """
    counter = 0  # í”„ë ˆì„ ì¹´ìš´í„° ì´ˆê¸°í™”

    try:
        while True:
            # ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ì„ ì½ìŒ
            res, frame = video_capture.read()
            if not res:
                print("ë¹„ë””ì˜¤ ëë‚¨")
                break

            # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # íƒ€ì¼ ê²°í•© ì´ë¯¸ì§€ ì´ˆê¸°í™”
            tiles_comb = None

            # ì§€ì •ëœ í”„ë ˆì„ ê°„ê²©ì— ë§ì¶° íƒ€ì¼ì„ ìƒì„±
            if counter % int(fps * frame_int) == 0:
                tiles, wn, hn = preprocess_input_image(thread_var.model, origin_image=frame, tile_size=tile_size, margin_size=tile_margin)

                if wn > 1 or hn > 1:
                    tiles_comb = combine_image_tiles(tiles, wn, hn)
                else:
                    tiles_comb = frame

                st.session_state.tiled_images = tiles

                # ìŠ¤ë ˆë“œ ë½ì„ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ íë¥¼ ì—…ë°ì´íŠ¸
                print("íƒ€ì¼ì´ ì–´ë–¤ í˜•íƒœëƒ!" , tiles[0].shape ,"--", len(tiles))
                with thread_var.thread_lock:
                    if len(thread_var.frame_queue) != frame_len:
                        mask = np.zeros_like(tiles)
                        for _ in range(frame_len):
                            thread_var.frame_queue.append(mask)
                    thread_var.frame_queue.append(tiles)
                    thread_var.frame_queue_updated = True

                # ëª¨ë¸ ìŠ¤ë ˆë“œì— í”„ë ˆì„ íê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŒì„ ì•Œë¦¼
                thread_var.frame_queue_updated_event.set()

                # ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                thread_var.model_processing_done_event.wait()
                thread_var.model_processing_done_event.clear()
                st.session_state.tiled_frame_com.write(f"{counter}")
                update_graph(thread_var.out_sim_scores)

            # í”„ë ˆì„ í¬ê¸° ì¡°ì • í›„ UI ì—…ë°ì´íŠ¸
            source_frame = st.session_state.video_output_frame
            frame = cv2.resize(frame, (MAX_WIDTH, MAX_HEIGHT), interpolation=cv2.INTER_LINEAR)
            source_frame.image(frame, use_column_width=True)

            # ë”œë ˆì´ ì„¤ì •ì´ ë˜ì–´ ìˆì„ ê²½ìš° ì‹œê°„ ëŒ€ê¸°
            if delay_state:
                # time.sleep(1.0 / fps - 0.01)
                time.sleep(0.05)

            # í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€
            counter += 1

    except Exception as e:
        print(f'error in : {e}')

    finally:
        # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ í•´ì œ
        video_capture.release()

# Execute model
def start_process(thread_var: ThreadManager, model_type, weight_path):
    
    # check video
    video_path = st.session_state.video_path
    if not video_path:
        print('error: video is not uploaded')
    
    # kill thread if running
    if thread_var.thread_enabled or st.session_state.model_thread is not None:
        kill_model_thread(thread_var)

    # kill model if changed
    if model_type != st.session_state.model_type or weight_path != st.session_state.model_weight_path:
        st.session_state.model_type = model_type
        st.session_state.model_weight_path = weight_path
        kill_model(thread_var)

    # load model
    model = prepare_model()

    thread_var.model = model

    # encode all prompts and save
    texts = st.session_state.prompt_all_text_list
    if texts:
        thread_var.text_vectors = thread_var.model(text = texts)
        print("í…ìŠ¤íŠ¸ ë²¡í„° ì‰ì…",thread_var.text_vectors.shape)

    # get video settings
    frame_len = st.session_state.frame_len
    frame_int = st.session_state.frame_int
    tile_size = st.session_state.tile_size
    tile_margin = st.session_state.tile_margin
    
    # make Video capture
    video_path = st.session_state.video_path
    delay_state = not isinstance(video_path, str) or not video_path.startswith("rtsp")

    video_capture = cv2.VideoCapture(video_path)
    st.session_state.video_capture = video_capture

    fps = video_capture.get(cv2.CAP_PROP_FPS)
    counter = 0
 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

    # start model thread
    start_model_thread(model, thread_var)
    video_processing_thread(video_capture, thread_var, fps, frame_int, tile_size, tile_margin, frame_len, MAX_WIDTH, MAX_HEIGHT, delay_state)



def record_video(queue:Queue, save_dir_path:str, fps):
    frame = queue.get() 
    h,w = frame.shape[:2]
    fourcc = cv2.VideoWriter_fourcc(*'XVID')

    frame_cnt = 0 
    Path(save_dir_path).mkdir(exist_ok=True, parents=True)
    while True :
        if frame_cnt % int(fps * 60 * 30) == 0 :  # 30ë¶„ ë§ˆë‹¤ ì €ì¥ 
            save_file_name = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
            save_path = os.path.join(save_dir_path, save_file_name + ".avi")
            vid_out = cv2.VideoWriter(save_path, fourcc=fourcc, 
                              fps=fps, frameSize=(w,h))
            frame_cnt = 1
        
        frame = queue.get()
        vid_out.write(frame)
        frame_cnt += 1


def start_btn():
    model_type = st.session_state.model_type
    model_weight_path = st.session_state.model_weight_path

    # st.markdown(
    #     """
    #     <style>
    #     button {
    #         height: auto;
    #         padding-top: 20px !important;
    #         padding-bottom: 20px !important;
    #         width: 100% !important; /* ë²„íŠ¼ì„ ê°€ë¡œë¡œ ê¸¸ê²Œ ëŠ˜ë¦¬ê¸° */
    #     }
    #     </style>
    #     """,
    #     unsafe_allow_html=True,
    # )
    start_btn = st.button(f"# Start", key="st_btn_run", type="primary", disabled=st.session_state.thread_enabled)
    
    if start_btn:
        # Initializing the session state
        st.session_state.prompt_score_dict = {}
        # Start process
        thread_var = ThreadManager()
        start_process(thread_var, model_type, model_weight_path)

